{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BuddhiGamage/Sinhala-Word-Embeddings-Trainer/blob/feature%2Ftrain-embedding/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXyzbB_fCRTN"
      },
      "source": [
        "\n",
        "\n",
        "# Word Embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UGgDrtmi-zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffafa30a-35bf-47aa-ee02-9d47de407ddf"
      },
      "source": [
        "#Start by connecting gdrive into the google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P1WT_w3Cem3"
      },
      "source": [
        "## Pre Proccessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtm22KFUC7F1"
      },
      "source": [
        "## Masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzGJSe-DAh0"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEO0HQzvBR1U"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXnBZozIDUJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bca67d8-2c93-4859-8473-c961e1137b6c"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "#from glove import Corpus, Glove\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi9m1v0eBWQH"
      },
      "source": [
        "Iterate through txt files in the given path . \n",
        "Read the files and return the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lMkGEY7IvA6"
      },
      "source": [
        "def get_training_data(rootdir):\n",
        "   \"\"\"\n",
        "  Iterate through folders sub folders\n",
        "  rootdir: directory where files locate\n",
        "  \n",
        "  Read the files in each path line by line.\n",
        "  Each sentence is tokenized\n",
        "  Returns:\n",
        "    sentence_list = list of tokenized sentences.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  file_list=[]\n",
        "  for root, dirs, files in os.walk(rootdir):\n",
        "        for name in files:\n",
        "            file_list.append(os.path.join(root, name))\n",
        "\n",
        "   \n",
        "  sentence_list = []\n",
        "  tokenize  = lambda x: nltk.word_tokenize(x)\n",
        "\n",
        "\n",
        "  for file_name in file_list:\n",
        "    with open(file_name, 'r', encoding=\"utf-8\") as text_file:\n",
        "      text = text_file.readlines()\n",
        "      for line in text:\n",
        "        #Preprocess function\n",
        "        #line = preprocess_text(line)\n",
        "        sentence_list.append(tokenize(line))\n",
        "\n",
        "  print('Found %s texts.' % len(sentence_list))\n",
        "\n",
        "  return sentence_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPg0QUkpO2z3",
        "outputId": "e9459eb1-7619-4fd9-f306-a3f308b4f56e"
      },
      "source": [
        "list2= get_training_data('/content/gdrive/MyDrive/Subasa V3.0 /Notebooks/Test Data',)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 37 texts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNAPdJbsBz3J"
      },
      "source": [
        "### Train the FastText and Wordvec word embediings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u95VANPtsZg7"
      },
      "source": [
        "\n",
        "def train(train_data_path, iter=500, sg=0, window=10, model='fasttext'):\n",
        "    \"\"\"\n",
        "    Train word embedding model\n",
        "    Args:\n",
        "        train_data_path: corpus path\n",
        "        iter: number of iterations\n",
        "        window: Window size\n",
        "        sg: 1 for skip gram 0 for CBOW\n",
        "        model: word embedding algorithms. Either 'fasttext' or 'word2vec'\n",
        "    Returns:\n",
        "        model: word embedding model\n",
        "    \"\"\"\n",
        "    train_data = get_training_data(train_data_path)\n",
        "    \n",
        "\n",
        "    #if you want you can save the processed corpus into seperate file in the disk so that you can save your time next time training\n",
        "    pickle_out = open(\"train_data.pickle\",\"wb\")\n",
        "    pickle.dump(train_data, pickle_out)\n",
        "    pickle_out.close()  \n",
        "    \n",
        "    assert model == 'fasttext' or model == 'word2vec', \"model must be fasttext or word2vec\"\n",
        "\n",
        "    if model == 'fasttext':\n",
        "        print('Training model', model,'.....')\n",
        "        # build vocabulary and train model\n",
        "        model = FastText(\n",
        "            train_data,\n",
        "            size=300,\n",
        "            window=window,\n",
        "            min_count=5,\n",
        "            workers=10,\n",
        "            iter=iter,\n",
        "            sg=sg)\n",
        "        return model\n",
        "\n",
        "    elif model == 'word2vec':\n",
        "        print('Training model', model, '.....')\n",
        "        # build vocabulary and train model\n",
        "        model = Word2Vec(\n",
        "            train_data,\n",
        "            size=300,\n",
        "            window=window,\n",
        "            min_count=5,\n",
        "            workers=10,\n",
        "            iter=iter,\n",
        "            sg=sg)\n",
        "        return model\n",
        "\n",
        "\n",
        "def save_embedding_model(model,model_dir,model_name):\n",
        "    \"\"\"\n",
        "    Save trained embedding model\n",
        "    Args:\n",
        "        model: word embedding model to save\n",
        "        model_dir: word embedding model directory\n",
        "        model_name: model name\n",
        "    \"\"\"\n",
        "    model_dir = Path(model_dir)\n",
        "   \n",
        "    if not model_dir.exists():\n",
        "        model_dir.mkdir()\n",
        "    print('Saving model in ', model_dir, ' .....')\n",
        "    model.save(os.path.join(model_dir,'{}.bin'.format(model_name)))\n",
        "    model.wv.save_word2vec_format(os.path.join(model_dir,'{}.txt'.format(model_name)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jehCr9yCiZV7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o0e4oLYoHAr",
        "outputId": "cb231eb7-50c0-48cd-cadb-c81697cbd139"
      },
      "source": [
        "model = train('/content/gdrive/MyDrive/Subasa V3.0 /Notebooks/Test Data', iter=20,sg=0, window=10, model='fasttext')\n",
        "save_embedding_model(model,'./model2','sin')\n",
        "#print ('Elapsed Time:', str(timedelta(seconds=(time.time()-start_time)))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model fasttext .....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqtTCApq0acr",
        "outputId": "8cb5fab8-f752-4ec4-a73a-a2ceb385fb9d"
      },
      "source": [
        "model = FastText.load('/content/model2/sin.bin')\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FastText(vocab=139940, size=300, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfTK5-wi2LZL",
        "outputId": "0d19db81-073b-4e54-adc6-5ae05e9f1906"
      },
      "source": [
        "model.most_similar('රජය',topn=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('රජයද', 0.8568500280380249),\n",
              " ('රජයම', 0.8442015647888184),\n",
              " ('රජයන', 0.7830795049667358),\n",
              " ('ආණ්ඩුව', 0.7629334926605225),\n",
              " ('රජයෙ', 0.7617363333702087),\n",
              " ('රජයටද', 0.7422264218330383),\n",
              " ('රජයන්', 0.7372043132781982),\n",
              " ('රජයත්', 0.7369410991668701),\n",
              " ('රජයයන්', 0.7320923805236816),\n",
              " ('රජයේද', 0.7236485481262207)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPhEjhTpDVjy"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUMkESTrDi0D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}